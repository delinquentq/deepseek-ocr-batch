#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†ç³»ç»Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½å’Œæ€§èƒ½
"""

import os
import sys
import json
import time
import asyncio
import tempfile
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from batch_pdf_processor import BatchPDFProcessor, DeepSeekOCRBatchProcessor, OpenRouterProcessor, JSONSchemaValidator
    from config_batch import Config, setup_environment
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…")
    sys.exit(1)

class Colors:
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    CYAN = '\033[36m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

class BatchSystemTester:
    """æ‰¹é‡å¤„ç†ç³»ç»Ÿæµ‹è¯•å™¨"""

    def __init__(self):
        self.config = Config()
        self.test_results = {}
        self.start_time = None

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print(f"{Colors.BLUE}{'='*60}")
        print(f"ğŸ§ª DeepSeek OCR æ‰¹é‡å¤„ç†ç³»ç»Ÿæµ‹è¯•")
        print(f"{'='*60}{Colors.RESET}\n")

        self.start_time = time.time()

        tests = [
            ("ç¯å¢ƒé…ç½®æµ‹è¯•", self.test_environment),
            ("é…ç½®éªŒè¯æµ‹è¯•", self.test_config_validation),
            ("JSON Schemaæµ‹è¯•", self.test_json_schema_validation),
            ("APIè¿æ¥æµ‹è¯•", self.test_api_connectivity),
            ("GPUå†…å­˜æµ‹è¯•", self.test_gpu_memory),
            ("æ¨¡å‹åŠ è½½æµ‹è¯•", self.test_model_loading),
            ("å›¾åƒå¤„ç†æµ‹è¯•", self.test_image_processing),
            ("ç«¯åˆ°ç«¯æµ‹è¯•", self.test_end_to_end)
        ]

        for test_name, test_func in tests:
            print(f"{Colors.CYAN}ğŸ”¬ {test_name}...{Colors.RESET}")
            try:
                result = test_func()
                self.test_results[test_name] = {"status": "PASS", "details": result}
                print(f"{Colors.GREEN}âœ… {test_name}: é€šè¿‡{Colors.RESET}\n")
            except Exception as e:
                self.test_results[test_name] = {"status": "FAIL", "error": str(e)}
                print(f"{Colors.RED}âŒ {test_name}: å¤±è´¥ - {e}{Colors.RESET}\n")

        self.generate_test_report()
        return self.test_results

    def test_environment(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¯å¢ƒé…ç½®"""
        results = {}

        # æ£€æŸ¥Pythonç‰ˆæœ¬
        import sys
        python_version = sys.version_info
        results["python_version"] = f"{python_version.major}.{python_version.minor}.{python_version.micro}"

        if python_version < (3, 8):
            raise Exception(f"Pythonç‰ˆæœ¬è¿‡ä½: {results['python_version']} (éœ€è¦ >= 3.8)")

        # æ£€æŸ¥CUDA
        try:
            import torch
            results["torch_version"] = torch.__version__
            results["cuda_available"] = torch.cuda.is_available()

            if torch.cuda.is_available():
                results["gpu_name"] = torch.cuda.get_device_name(0)
                results["gpu_memory"] = torch.cuda.get_device_properties(0).total_memory / 1024**3
            else:
                raise Exception("CUDAä¸å¯ç”¨")

        except ImportError:
            raise Exception("PyTorchæœªå®‰è£…")

        # æ£€æŸ¥å…³é”®æ¨¡å—
        required_modules = [
            "vllm", "transformers", "PIL", "aiohttp", "jsonschema",
            "numpy", "tqdm", "fitz", "img2pdf"
        ]

        missing_modules = []
        for module in required_modules:
            try:
                __import__(module)
                results[f"{module}_available"] = True
            except ImportError:
                missing_modules.append(module)
                results[f"{module}_available"] = False

        if missing_modules:
            raise Exception(f"ç¼ºå°‘å¿…éœ€æ¨¡å—: {missing_modules}")

        return results

    def test_config_validation(self) -> Dict[str, Any]:
        """æµ‹è¯•é…ç½®éªŒè¯"""
        results = {}

        try:
            # è®¾ç½®ç¯å¢ƒ
            setup_environment()
            results["environment_setup"] = "SUCCESS"

            # éªŒè¯é…ç½®
            self.config.validate_config()
            results["config_validation"] = "SUCCESS"

            # æ£€æŸ¥å…³é”®è·¯å¾„
            critical_paths = {
                "schema_path": self.config.paths.SCHEMA_PATH,
                "input_dir": self.config.paths.INPUT_DIR,
                "output_dir": self.config.paths.OUTPUT_DIR
            }

            for name, path in critical_paths.items():
                if path.exists():
                    results[name] = "EXISTS"
                else:
                    results[name] = "MISSING"

        except Exception as e:
            raise Exception(f"é…ç½®éªŒè¯å¤±è´¥: {e}")

        return results

    def test_json_schema_validation(self) -> Dict[str, Any]:
        """æµ‹è¯•JSON SchemaéªŒè¯"""
        results = {}

        # æµ‹è¯•SchemaåŠ è½½
        try:
            validator = JSONSchemaValidator(str(self.config.paths.SCHEMA_PATH))
            results["schema_loading"] = "SUCCESS"
        except Exception as e:
            raise Exception(f"SchemaåŠ è½½å¤±è´¥: {e}")

        # æµ‹è¯•æœ‰æ•ˆJSONéªŒè¯
        valid_json = self._create_test_json()
        is_valid, error = validator.validate(valid_json)
        results["valid_json_test"] = "PASS" if is_valid else f"FAIL: {error}"

        # æµ‹è¯•æ— æ•ˆJSONéªŒè¯
        invalid_json = {"invalid": "data"}
        is_valid, error = validator.validate(invalid_json)
        results["invalid_json_test"] = "PASS" if not is_valid else "FAIL: Should reject invalid JSON"

        # æµ‹è¯•JSONä¿®å¤åŠŸèƒ½
        fixed_json, warnings = validator.validate_and_fix(invalid_json)
        results["json_fix_test"] = f"PASS: {len(warnings)} warnings"

        return results

    async def test_api_connectivity(self) -> Dict[str, Any]:
        """æµ‹è¯•APIè¿æ¥"""
        results = {}

        if not self.config.api.OPENROUTER_API_KEY:
            raise Exception("OPENROUTER_API_KEYæœªè®¾ç½®")

        try:
            async with OpenRouterProcessor() as processor:
                # æµ‹è¯•ç®€å•çš„APIè°ƒç”¨
                test_messages = [
                    {"role": "user", "content": "Hello, this is a test. Please respond with 'Test successful'."}
                ]

                for model_key in ["gemini", "qwen"]:
                    try:
                        start_time = time.time()
                        response = await processor.call_model(model_key, test_messages, max_tokens=100)
                        end_time = time.time()

                        if response and 'choices' in response:
                            results[f"{model_key}_api"] = {
                                "status": "SUCCESS",
                                "response_time": f"{end_time - start_time:.2f}s",
                                "usage": response.get('usage', {})
                            }
                        else:
                            results[f"{model_key}_api"] = {"status": "FAIL", "error": "Invalid response"}

                    except Exception as e:
                        results[f"{model_key}_api"] = {"status": "FAIL", "error": str(e)}

        except Exception as e:
            raise Exception(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")

        return results

    def test_gpu_memory(self) -> Dict[str, Any]:
        """æµ‹è¯•GPUå†…å­˜ä½¿ç”¨"""
        results = {}

        try:
            import torch

            if not torch.cuda.is_available():
                raise Exception("CUDAä¸å¯ç”¨")

            # è·å–åˆå§‹å†…å­˜çŠ¶æ€
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory

            results["initial_memory_mb"] = initial_memory / 1024**2
            results["total_memory_gb"] = total_memory / 1024**3

            # æµ‹è¯•å†…å­˜åˆ†é…
            test_tensor = torch.randn(1000, 1000, device='cuda')
            allocated_memory = torch.cuda.memory_allocated(0)
            results["test_allocation_mb"] = (allocated_memory - initial_memory) / 1024**2

            # æ¸…ç†
            del test_tensor
            torch.cuda.empty_cache()

            # æ£€æŸ¥å¯ç”¨å†…å­˜æ˜¯å¦è¶³å¤Ÿ
            available_memory_gb = (total_memory - initial_memory) / 1024**3
            results["available_memory_gb"] = available_memory_gb

            if available_memory_gb < 18:
                raise Exception(f"å¯ç”¨æ˜¾å­˜ä¸è¶³: {available_memory_gb:.1f}GB (æ¨è >= 18GB)")

        except Exception as e:
            raise Exception(f"GPUå†…å­˜æµ‹è¯•å¤±è´¥: {e}")

        return results

    def test_model_loading(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹åŠ è½½"""
        results = {}

        try:
            # æµ‹è¯•DeepSeek OCRå¤„ç†å™¨åˆå§‹åŒ–
            start_time = time.time()
            processor = DeepSeekOCRBatchProcessor()
            end_time = time.time()

            results["model_loading_time"] = f"{end_time - start_time:.2f}s"
            results["model_loading"] = "SUCCESS"

            # æ£€æŸ¥æ¨¡å‹ç»„ä»¶
            if hasattr(processor, 'llm') and processor.llm is not None:
                results["llm_initialized"] = True
            else:
                results["llm_initialized"] = False

            if hasattr(processor, 'sampling_params'):
                results["sampling_params_configured"] = True
            else:
                results["sampling_params_configured"] = False

        except Exception as e:
            raise Exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        return results

    def test_image_processing(self) -> Dict[str, Any]:
        """æµ‹è¯•å›¾åƒå¤„ç†"""
        results = {}

        try:
            from PIL import Image
            import numpy as np

            # åˆ›å»ºæµ‹è¯•å›¾åƒ
            test_image = Image.new('RGB', (640, 480), color='white')

            # æ·»åŠ ä¸€äº›å†…å®¹åˆ°å›¾åƒ
            from PIL import ImageDraw, ImageFont
            draw = ImageDraw.Draw(test_image)
            font = ImageFont.load_default()
            draw.text((10, 10), "Test Document", fill='black', font=font)
            draw.rectangle([50, 50, 200, 100], outline='red', width=2)

            results["test_image_created"] = True

            # æµ‹è¯•å›¾åƒé¢„å¤„ç†
            processor = DeepSeekOCRBatchProcessor()
            batch_inputs = processor.process_images_batch([test_image])

            if batch_inputs and len(batch_inputs) == 1:
                results["image_preprocessing"] = "SUCCESS"
                results["batch_size"] = len(batch_inputs)
            else:
                results["image_preprocessing"] = "FAIL"

        except Exception as e:
            raise Exception(f"å›¾åƒå¤„ç†æµ‹è¯•å¤±è´¥: {e}")

        return results

    async def test_end_to_end(self) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯æµ‹è¯•"""
        results = {}

        try:
            # åˆ›å»ºä¸´æ—¶PDFæ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
            test_content = """
            # æµ‹è¯•æ–‡æ¡£

            è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯æ‰¹é‡å¤„ç†ç³»ç»Ÿã€‚

            ## è´¢åŠ¡æ•°æ®

            - æ”¶å…¥: $100M
            - åˆ©æ¶¦: $20M
            - å¢é•¿ç‡: 15%

            ## å›¾è¡¨æ•°æ®

            | å­£åº¦ | æ”¶å…¥ | åˆ©æ¶¦ |
            |------|------|------|
            | Q1   | 25M  | 5M   |
            | Q2   | 30M  | 6M   |
            """

            # ç”±äºåˆ›å»ºçœŸå®PDFæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œåªæµ‹è¯•JSONç”Ÿæˆéƒ¨åˆ†
            test_json = self._create_test_json()

            # æµ‹è¯•JSONéªŒè¯
            validator = JSONSchemaValidator(str(self.config.paths.SCHEMA_PATH))
            is_valid, error = validator.validate(test_json)

            if is_valid:
                results["json_validation"] = "PASS"
            else:
                results["json_validation"] = f"FAIL: {error}"

            # æµ‹è¯•APIè°ƒç”¨ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
            if self.config.api.OPENROUTER_API_KEY:
                async with OpenRouterProcessor() as processor:
                    test_messages = [{"role": "user", "content": "Extract data from: Test Revenue: $100M"}]

                    try:
                        response = await processor.call_model("gemini", test_messages, max_tokens=200)
                        results["api_integration"] = "SUCCESS"
                    except Exception as e:
                        results["api_integration"] = f"FAIL: {e}"
            else:
                results["api_integration"] = "SKIPPED: No API key"

        except Exception as e:
            raise Exception(f"ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")

        return results

    def _create_test_json(self) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•JSONæ•°æ®"""
        return {
            "_id": "test_document_12345",
            "source": {
                "file_name": "test_document.pdf",
                "processing_metadata": {
                    "vision_model": "test-vision-model",
                    "synthesis_model": "test-synthesis-model",
                    "validation_model": "test-validation-model",
                    "processed_at": "2024-10-21T10:00:00Z",
                    "pages_processed": 1,
                    "successful_pages": 1
                }
            },
            "report": {
                "title": "Test Financial Report",
                "report_date": "2024-10-21",
                "report_type": "company",
                "symbols": ["TEST"],
                "sector": "Technology",
                "content": "This is a test report content.",
                "word_count": 100
            },
            "data": {
                "figures": [{
                    "figure_id": "test_revenue_chart",
                    "type": "bar_chart",
                    "title": "Revenue Growth",
                    "description": "Quarterly revenue growth",
                    "data": {
                        "labels": ["Q1", "Q2", "Q3", "Q4"],
                        "series": [{
                            "name": "Revenue",
                            "values": [100, 120, 135, 150],
                            "unit": "$M"
                        }]
                    },
                    "source_page": 1
                }],
                "numerical_data": [{
                    "value": "150",
                    "figure_id": "test_revenue_chart",
                    "context": "Q4 revenue",
                    "metric_type": "currency",
                    "source_page": 1
                }],
                "companies": ["Test Company"],
                "key_metrics": ["revenue", "growth"],
                "extraction_summary": {
                    "figures_count": 1,
                    "numerical_data_count": 1,
                    "companies_mentioned": 1,
                    "figures_with_linked_data": 1,
                    "validation_summary": {
                        "original_figures": 1,
                        "kept_figures": 1,
                        "original_numerical": 1,
                        "kept_numerical": 1,
                        "validation_method": "test_validation",
                        "data_accuracy_rate": 100.0
                    }
                }
            },
            "query_capabilities": {
                "description": "Test document with query capabilities",
                "searchable_fields": ["report.title", "data.figures.title"],
                "figure_data_available": True,
                "can_recreate_charts": True
            }
        }

    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        end_time = time.time()
        total_time = end_time - self.start_time

        print(f"\n{Colors.BLUE}{'='*60}")
        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*60}{Colors.RESET}")

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result["status"] == "PASS")
        failed_tests = total_tests - passed_tests

        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"é€šè¿‡: {Colors.GREEN}{passed_tests}{Colors.RESET}")
        print(f"å¤±è´¥: {Colors.RED}{failed_tests}{Colors.RESET}")
        print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print()

        # è¯¦ç»†ç»“æœ
        for test_name, result in self.test_results.items():
            status_color = Colors.GREEN if result["status"] == "PASS" else Colors.RED
            print(f"{status_color}{result['status']:<6}{Colors.RESET} {test_name}")

            if result["status"] == "FAIL":
                print(f"         é”™è¯¯: {result['error']}")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = Path("test_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": time.time(),
                "total_time": total_time,
                "summary": {
                    "total": total_tests,
                    "passed": passed_tests,
                    "failed": failed_tests
                },
                "results": self.test_results
            }, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # æ ¹æ®ç»“æœå†³å®šé€€å‡ºä»£ç 
        if failed_tests > 0:
            print(f"\n{Colors.RED}âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯{Colors.RESET}")
            return False
        else:
            print(f"\n{Colors.GREEN}âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨{Colors.RESET}")
            return True

async def main():
    """ä¸»å‡½æ•°"""
    print(f"{Colors.BOLD}DeepSeek OCR æ‰¹é‡å¤„ç†ç³»ç»Ÿæµ‹è¯•{Colors.RESET}\n")

    tester = BatchSystemTester()
    success = await asyncio.get_event_loop().run_in_executor(None, tester.run_all_tests)

    return success

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}âš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­{Colors.RESET}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Colors.RED}âŒ æµ‹è¯•å¤±è´¥: {e}{Colors.RESET}")
        sys.exit(1)